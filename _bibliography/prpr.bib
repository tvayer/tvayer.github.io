%%%%%%%%%%%%
% Sample  arXiv
ARTICLE{aobc18,
  title         = "High Dimensional Data Enrichment: Interpretable, Fast, and
                   {Data-Efficient}",
  author        = "Asiaee, Amir and Oymak, Samet and Coombes, Kevin R and
                   Banerjee, Arindam",
  journal = "\textnormal{Under review in}",
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1806.04047",
  abstract={High dimensional structured data enriched model describes groups of observations by shared and per-group individual parameters, each with its own structure such as sparsity or group sparsity. In this paper, we consider the general form of data enrichment where data comes in a fixed but arbitrary number of groups G.  Any convex function, e.g., norms, can characterize the structure of both shared and individual parameters. We propose an estimator for high dimensional data enriched model and provide conditions under which it consistently estimates both shared and individual parameters. We also delineate sample complexity of the estimator and present high probability non-asymptotic bound on estimation error of all parameters. Interestingly the sample complexity of our estimator translates to conditions on both per-group sample sizes and the total number of samples. We propose an iterative estimation algorithm with linear convergence rate and supplement our theoretical analysis with synthetic and real experimental results. Particularly, we show the predictive power of data enriched model along with its interpretable results in anticancer drug sensitivity analysis.}, 
  note = "https://arxiv.org/abs/1806.04047"
}
%%%%%%%%%%%%


@article{Vaysketch,
      title={{Controlling Wasserstein distances by Kernel norms with application to Compressive Statistical Learning}}, 
      author={Vayer, Titouan and Rémi Gribonval},
      YEAR={2021},
      abstract = {Comparing probability distributions is at the crux of many machine learning algorithms. Maximum Mean Discrepancies (MMD) and Optimal Transport distances (OT) are two classes of distances between probability measures that have attracted abundant attention in past years. This paper establishes some conditions under which the Wasserstein distance can be controlled by MMD norms. Our work is motivated by the compressive statistical learning (CSL) theory, a general framework for resource-efficient large scale learning in which the training data is summarized in a single vector (called sketch) that captures the information relevant to the considered learning task. Inspired by existing results in CSL, we introduce the Hölder Lower Restricted Isometric Property (Hölder LRIP) and show that this property comes with interesting guarantees for compressive statistical learning. Based on the relations between the MMD and the Wasserstein distance, we provide guarantees for compressive statistical learning by introducing and studying the concept of Wasserstein learnability of the learning task, that is when some task-specific metric between probability distributions can be bounded by a Wasserstein distance.},
      note = {https://hal.archives-ouvertes.fr/hal-03461492},
      archivePrefix={arXiv},
      presort   = {13},

}


@article{Vaytimeseries,
    title={Time Series Alignment with Global Invariances},
    author={Vayer, Titouan and Laetitia Chapel and Nicolas Courty and Rémi Flamary and Yann Soullard and Romain Tavenard},
    year={2020},
    abstract = {In this work we address the problem of comparing time series while taking into account both feature space transformation and temporal variability. The proposed framework combines a latent global transformation of the feature space with the widely used Dynamic Time Warping (DTW). The latent global transformation captures the feature invariance while the DTW (or its smooth counterpart soft-DTW) deals with the temporal shifts. We cast the problem as a joint optimization over the global transformation and the temporal alignments. The versatility of our framework allows for several variants depending on the invariance class at stake. Among our contributions we define a differentiable loss for time series and present two algorithms for the computation of time series barycenters under our new geometry. We illustrate the interest of our approach on both simulated and real world data.},
    note = {https://hal.archives-ouvertes.fr/hal-02473959},
}



