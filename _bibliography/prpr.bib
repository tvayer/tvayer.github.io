%%%%%%%%%%%%
% Sample  arXiv
%%%%%%%%%%%%

@unpublished{lebeurrier,
  TITLE = {{Path-conditioned training: a principled way to rescale ReLU neural networks}},
  AUTHOR = {Lebeurrier, Arthur and Vayer, Titouan and Gribonval, Remi},
  URL = {https://hal.science/hal-05519191},
  YEAR = {2026},
  MONTH = {Jan},
  note = {https://hal.science/hal-05519191v1/file/path_dynamic_preprint.pdf},
  HAL_ID = {hal-05519191},
  HAL_VERSION = {v1},
  abstract = {Despite recent algorithmic advances, we still lack principled ways to leverage the well-documented rescaling symmetries in ReLU neural network parameters. While two properly rescaled weights implement the same function, the training dynamics can be dramatically different. To offer a fresh perspective on exploiting this phenomenon, we build on the recent path-lifting framework, which provides a compact factorization of ReLU networks. We introduce a geometrically motivated criterion to rescale neural network parameters which minimization leads to a conditioning strategy that aligns a kernel in the path-lifting space with a chosen reference. We derive an efficient algorithm to perform this alignment. In the context of random network initialization, we analyze how the architecture and the initialization scale jointly impact the output of the proposed method. Numerical experiments illustrate its potential to speed up training.}
}


@article{vayersparsityot,
  TITLE = {{On sparsity, extremal structure, and monotonicity properties of Wasserstein and Gromov-Wasserstein optimal transport plans}},
  AUTHOR = {Vayer, Titouan},
  YEAR = {2026},
  MONTH = {Feb},
  note = {https://hal.science/hal-05515773v2/file/gromov_simple.pdf},
  HAL_ID = {hal-05515773},
  HAL_VERSION = {v2},
  abstract = {This note gives a self-contained overview of some important properties of the Gromov-Wasserstein (GW) distance, compared with the standard linear optimal transport (OT) framework. More specifically, I explore the following questions: are GW optimal transport plans sparse? Under what conditions are they supported on a permutation? Do they satisfy a form of cyclical monotonicity? In particular, I present the conditionally negative semi-definite property and show that, when it holds, there are GW optimal plans that are sparse and supported on a permutation.}
}

@article{vayernote,
  TITLE = {{A note on the relations between mixture models, maximum-likelihood and entropic optimal transport}},
  AUTHOR = {Vayer, Titouan and Lasalle, Etienne},
  note = {https://hal.science/hal-04901607v2/file/main.pdf},
  YEAR = {2025},
  MONTH = {Jan},
  KEYWORDS = {Optimal transport ; EM algorithm and mixture models ; Likelihood Maximization},
  HAL_ID = {hal-04901607},
  HAL_VERSION = {v2},
  abstract = {This note aims to demonstrate that performing maximum-likelihood estimation for a mixture model is equivalent to minimizing over the parameters an optimal transport problem with entropic regularization. The objective is pedagogical: we seek to present this already known result in a concise and hopefully simple manner. We give an illustration with Gaussian mixture models by showing that the standard EM algorithm is a specific block-coordinate descent on an optimal transport loss.}
}



@article{vayercompressive,
  TITLE = {{Compressive Recovery of Sparse Precision Matrices}},
  AUTHOR = {Vayer, Titouan and Lasalle, Etienne and Gribonval, R{\'e}mi and Gon{\c c}alves, Paulo},
  abstract = {{We consider the problem of learning a graph modeling the statistical relations of the $d$ variables of a dataset with $n$ samples $X \in \mathbb{R}^{n \times d}$. Standard approaches amount to searching for a precision matrix $\Theta$ representative of a Gaussian graphical model that adequately explains the data. However, most maximum likelihood-based estimators usually require storing the $d^{2}$ values of the empirical covariance matrix, which can become prohibitive in a high-dimensional setting. In this work, we adopt a compressive viewpoint and aim to estimate a sparse $\Theta$ from a sketch of the data, i.e. a low-dimensional vector of size $m \ll d^{2}$ carefully designed from $X$ using nonlinear random features. Under certain assumptions on the spectrum of $\Theta$ (or its condition number), we show that it is possible to estimate it from a sketch of size $m=\Omega((d+2k)\log(d))$ where $k$ is the maximal number of edges of the underlying graph. These information-theoretic guarantees are inspired by compressed sensing theory and involve restricted isometry properties and instance optimal decoders. We investigate the possibility of achieving practical recovery with an iterative algorithm based on the graphical lasso, viewed as a specific denoiser. We compare our approach and graphical lasso on synthetic datasets, demonstrating its favorable performance even when the dataset is compressed.}},
  NOTE = {working paper or preprint},
  YEAR = {2023},
  MONTH = Nov,
  note = {https://arxiv.org/pdf/2311.04673},
  HAL_ID = {hal-04275341},
  HAL_VERSION = {v1},
}

