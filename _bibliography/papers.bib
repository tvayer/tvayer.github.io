@article{Vaysubspace,
AUTHOR = {Bonet, Clément and Vayer, Titouan and Courty, Nicolas and Septier, François and Drumetz, Lucas},
TITLE = {{Subspace Detours Meet Gromov–Wasserstein}},
JOURNAL = {Algorithms},
VOLUME = {14},
YEAR = {2021},
NUMBER = {12},
ISSN={1999-4893}, 
note={http://dx.doi.org/10.3390/a14120366}, 
abstract ={In the context of optimal transport (OT) methods, the subspace detour approach was recently proposed by Muzellec and Cuturi. It consists of first finding an optimal plan between the measures projected on a wisely chosen subspace and then completing it in a nearly optimal transport plan on the whole space. The contribution of this paper is to extend this category of methods to the Gromov–Wasserstein problem, which is a particular type of OT distance involving the specific geometry of each distribution. After deriving the associated formalism and properties, we give an experimental illustration on a shape matching problem. We also discuss a specific cost for which we can show connections with the Knothe–Rosenblatt rearrangement.},
month={Dec}, 
pages={366} 
}


@article{Vaypot,
  author  = {R\'{e}mi Flamary* and Nicolas Courty* and Alexandre Gramfort* and Mokhtar Z. Alaya and Aur\'{e}lie Boisbunon and Stanislas Chambon and Laetitia Chapel and Adrien Corenflos and Kilian Fatras and Nemo Fournier and L\'{e}o Gautheron and Nathalie T.H. Gayraud and Hicham Janati and Alain Rakotomamonjy and Ievgen Redko and Antoine Rolet and Antony Schutz and Vivien Seguy and Danica J. Sutherland and Romain Tavenard and Alexander Tong and Titouan Vayer},
  abstract = {Optimal transport has recently been reintroduced to the machine learning community thanks in part to novel efficient optimization procedures allowing for medium to large scale applications. We propose a Python toolbox that implements several key optimal transport ideas for the machine learning community. The toolbox contains implementations of a number of founding works of OT for machine learning such as Sinkhorn algorithm and Wasserstein barycenters, but also provides generic solvers that can be used for conducting novel fundamental research. This toolbox, named POT for Python Optimal Transport, is open source with an MIT license.},
  title   = {{POT: Python Optimal Transport}},
  note     = {http://jmlr.org/papers/v22/20-451.html},
  journal = {Journal of Machine Learning Research (JMLR)},
  year    = {2021},
  volume  = {22},
  number  = {78},
  pages   = {1-8},
  presort   = {02},
}




@article{Vayerfgwalgorithms, 
  title={{Fused Gromov-Wasserstein distance for structured objects}},
  author={Vayer, Titouan and Chapel, Laetitia and Flamary, R{\'e}mi and Tavenard, Romain and Courty, Nicolas},
  journal={Algorithms},
  volume = {13},
  number={9},
  pages={212},
  year={2020},
  publisher={Multidisciplinary Digital Publishing Institute},
  ISSN={1999-4893},
  note={http://dx.doi.org/10.3390/a13090212},
  DOI={10.3390/a13090212},
  month={Aug}
}

%%% Conf

@inproceedings{Vaysemirelaxed,
title={{Semi-relaxed Gromov-Wasserstein divergence and applications on graphs}},
author={C{\'e}dric Vincent-Cuaz and R{\'e}mi Flamary and Marco Corneli and Titouan Vayer and Nicolas Courty},
booktitle={International Conference on Learning Representations (ICLR)},
year={2022},
note = {https://openreview.net/forum?id=RShaMexjc-x},
abstract = {Comparing structured objects such as graphs is a fundamental operation
involved in many learning tasks. To this end, the Gromov-Wasserstein (GW)
distance, based on Optimal Transport (OT), has proven to be successful in
handling the specific nature of the associated objects. More specifically,
through the nodes connectivity relations, GW operates on graphs, seen as
probability measures over specific spaces. At the core of OT is the idea of
conservation of mass, which imposes a coupling between all the nodes from
the two considered graphs. We argue in this paper that this property can be
detrimental for tasks such as graph dictionary or partition learning, and we
relax it by proposing a new semi-relaxed Gromov-Wasserstein divergence.
Aside from immediate computational benefits, we discuss its properties, and
show that it can lead to an efficient graph dictionary learning algorithm.
We empirically demonstrate its relevance for complex tasks on graphs such as
partitioning, clustering and completion.},
ADDRESS = {Online}
}

@inproceedings{Vaymultiscale,
      title={{Fast Multiscale Diffusion on Graphs}}, 
      author={Sibylle Marcotte and Amélie Barbe and Rémi Gribonval and Vayer, Titouan and Marc Sebban and Pierre Borgnat and Paulo Gonçalves},
      year={2022},
      booktitle={International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
      note = {https://hal.archives-ouvertes.fr/hal-03212764},
      ADDRESS = {Singapore, Singapore},
      abstract = {Diffusing a graph signal at multiple scales requires computing the action of the exponential of several multiples of the Laplacian matrix. We tighten a bound on the approximation error of truncated Chebyshev polynomial approximations of the exponential, hence significantly improving a priori estimates of the polynomial order for a prescribed error. We further exploit properties of these approximations to factorize the computation of the action of the diffusion operator over multiple scales, thus reducing drastically its computational cost.},
      YEAR = {2022},
      MONTH = May
}





@inproceedings{Vaydiff,
  TITLE = {{Optimization of the Diffusion Time in Graph Diffused-Wasserstein Distances: Application to Domain Adaptation}},
  AUTHOR = {Barbe, Am{\'e}lie and Gon{\c c}alves, Paulo and Sebban, Marc and Borgnat, Pierre and Gribonval, R{\'e}mi and Vayer, Titouan},
  BOOKTITLE = {{IEEE 33rd International Conference on Tools with Artificial Intelligence (ICTAI)}},
  YEAR = {2021},
  volume={},
  number={},
  pages={786-790},
  ADDRESS = {Online},
  abstract = {The use of the heat kernel on graphs has recently given rise to a family of so-called Diffusion-Wasserstein distances which resort to Optimal Transport theory for comparing attributed graphs. In this paper, we address the open problem of optimizing the diffusion time used in these distances. Inspired from the notion of triplet-based constraints, we design a loss function that aims at bringing two graphs closer together while keeping an impostor away. After a thorough analysis of the properties of this function, we show on synthetic data that the resulting Diffusion-Wasserstein distances outperforms the Gromov and Fused-Gromov Wasserstein distances on unsupervised graph domain adaptation tasks.}
}

@inproceedings{Vaygdl,
  title =    {{Online Graph Dictionary Learning}},
  author =       {Vincent-Cuaz, C{\'e}dric and Vayer, Titouan and Flamary, R{\'e}mi and Corneli, Marco and Courty, Nicolas},
  booktitle =    {International Conference on Machine Learning (ICML)},
  year =   {2021},
  pages =    {10564--10574},
  volume =   {139},
  ADDRESS = {Online},
  series =   {Proceedings of Machine Learning Research},
  month =    {18--24 Jul},
  publisher =    {PMLR},
  note =    {https://proceedings.mlr.press/v139/vincent-cuaz21a.html},
  abstract = {Dictionary learning is a key tool for representation learning, that explains the data as linear combination of few basic elements. Yet, this analysis is not amenable in the context of graph learning, as graphs usually belong to different metric spaces. We fill this gap by proposing a new online Graph Dictionary Learning approach, which uses the Gromov Wasserstein divergence for the data fitting term. In our work, graphs are encoded through their nodes’ pairwise relations and modeled as convex combination of graph atoms, i.e. dictionary elements, estimated thanks to an online stochastic algorithm, which operates on a dataset of unregistered graphs with potentially different number of nodes. Our approach naturally extends to labeled graphs, and is completed by a novel upper bound that can be used as a fast approximation of Gromov Wasserstein in the embedding space. We provide numerical evidences showing the interest of our approach for unsupervised embedding of graph datasets and for online graph subspace estimation and tracking.
}
}




@inproceedings{Vaycoot,
 author = {Titouan Vayer* and Ievgen Redko* and  R\'{e}mi Flamary* and Nicolas Courty*},
 booktitle = { Neural Information Processing Systems (NeurIPS)},
 title = {{CO-Optimal Transport}},
 year = {2020}, 
 ADDRESS = {Online, Canada},
 MONTH = {Dec},
 VOLUME = {33},
 MONTH = {Dec},
 note = {https://papers.nips.cc/paper/2020/hash/cc384c68ad503482fb24e6d1e3b512ae-Abstract.html},
 abstract = {Optimal transport (OT) is a powerful geometric and probabilistic tool for finding correspondences and measuring similarity between two distributions. Yet, its original formulation relies on the existence of a cost function between the samples of the two distributions, which makes it impractical when they are supported on different spaces. To circumvent this limitation, we propose a novel OT problem, named COOT for CO-Optimal Transport, that simultaneously optimizes two transport maps between both samples and features, contrary to other approaches that either discard the individual features by focusing on pairwise distances between samples or need to model explicitly the relations between them. We provide a thorough theoretical analysis of our problem, establish its rich connections with other OT-based distances and demonstrate its versatility with two machine learning applications in heterogeneous domain adaptation and co-clustering/data summarization, where COOT leads to performance improvements over the state-of-the-art methods.
}
}

@inproceedings{Vaysgw,
 author = {Vayer, Titouan and Flamary, R\'{e}mi and Courty, Nicolas and Tavenard, Romain and Chapel, Laetitia},
 booktitle = {Neural Information Processing Systems (NeurIPS)},
 title = {{Sliced Gromov-Wasserstein}},
 year = {2019},
 ADDRESS = {Vancouver, Canada},
 VOLUME = {32},
 note = {https://papers.nips.cc/paper/9615-sliced-gromov-wasserstein},
 abstract = {Recently used in various machine learning contexts, the Gromov-Wasserstein distance (GW) allows for comparing distributions whose supports do not necessarily lie in the same metric space. However, this Optimal Transport (OT) distance requires solving a complex non convex quadratic program which is most of the time very costly both in time and memory. Contrary to GW, the Wasserstein distance (W) enjoys several properties ({\em e.g.} duality) that permit large scale optimization. Among those, the solution of W on the real line, that only requires sorting discrete samples in 1D, allows defining the Sliced Wasserstein (SW) distance. This paper proposes a new divergence based on GW akin to SW. We first derive a closed form for GW when dealing with 1D distributions, based on a new result for the related quadratic assignment problem. We then define a novel OT discrepancy that can deal with large scale distributions via a slicing approach and we show how it relates to the GW distance while being $O(n\log(n))$ to compute. We illustrate the behavior of this so called Sliced Gromov-Wasserstein (SGW) discrepancy in experiments where we demonstrate its ability to tackle similar problems as GW while being several order of magnitudes faster to compute.
}
}




@inproceedings{Vayfgwicml,
  title =    {{Optimal Transport for structured data with application on graphs}},
  author =       {Vayer, Titouan and Courty, Nicolas and Tavenard, Romain and Chapel, Laetitia  and Flamary, R{\'e}mi},
  booktitle =    {International Conference on Machine Learning (ICML)},
  year =   {2019},
  pages =    {6275--6284},
  volume =   {97},
  series =   {Proceedings of Machine Learning Research},
  month =    {09--15 Jun},
  ADDRESS = {Long Beach, USA},
  note =    {https://proceedings.mlr.press/v97/titouan19a.html},
  abstract = {This work considers the problem of computing distances between structured objects such as undirected graphs, seen as probability distributions in a specific metric space. We consider a new transportation distance ( i.e. that minimizes a total cost of transporting probability masses) that unveils the geometric nature of the structured objects space. Unlike Wasserstein or Gromov-Wasserstein metrics that focus solely and respectively on features (by considering a metric in the feature space) or structure (by seeing structure as a metric space), our new distance exploits jointly both information, and is consequently called Fused Gromov-Wasserstein (FGW). After discussing its properties and computational aspects, we show results on a graph classification task, where our method outperforms both graph kernels and deep graph convolutional networks. Exploiting further on the metric properties of FGW, interesting geometric objects such as Fr{é}chet means or barycenters of graphs are illustrated and discussed in a clustering context.}
}



