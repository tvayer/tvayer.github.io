{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this work we will build simple and more complicated autoencoders on the MNIST dataset.\n",
    "\n",
    "An autoencoder is a neural network that is trained to attempt to copy its input to its output. It has two parts :\n",
    "\n",
    "\n",
    "- An encoder function $h_{\\theta_{e}} : \\mathcal{X} \\rightarrow \\mathcal{Z}$ that pushes the inputs $x$ in a smaller dimensional space.\n",
    "- A decoder function $g_{\\theta_{d}} : \\mathcal{Z} \\rightarrow \\mathcal{X}$ that reconstructs from the low dimensional space to the initial space\n",
    "\n",
    "Very generally autoencoders aim at solving  : \n",
    "\n",
    "$$\\underset{\\theta_{e},\\theta_{d}}{\\text{min}} \\ \\underset{x \\sim \\mathbb{P}_{r}}{\\mathbb{E}}[L(x,g_{\\theta_{d}},h_{\\theta_{e}})]$$\n",
    "\n",
    "<img src=\"imgs/autoencoder.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcol\n",
    "from matplotlib import cm\n",
    "def graph_colors(nx_graph):\n",
    "    #cm1 = mcol.LinearSegmentedColormap.from_list(\"MyCmapName\",[\"blue\",\"red\"])\n",
    "    #cm1 = mcol.Colormap('viridis')\n",
    "\n",
    "    cnorm = mcol.Normalize(vmin=0,vmax=9)\n",
    "    cpick = cm.ScalarMappable(norm=cnorm,cmap='Set1')\n",
    "    cpick.set_array([])\n",
    "    val_map = {}\n",
    "    for k,v in nx.get_node_attributes(nx_graph,'attr').items():\n",
    "        #print(v)\n",
    "        val_map[k]=cpick.to_rgba(v)\n",
    "    #print(val_map)\n",
    "    colors=[]\n",
    "    for node in nx_graph.nodes():\n",
    "        #print(node,val_map.get(str(node), 'black'))\n",
    "        colors.append(val_map[node])\n",
    "    return colors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the MNIST dataset using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
    "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1 Write a function that builds a simple autoencoder \n",
    "\n",
    "The autoencoder must have a simple Dense layer with relu activation. The number of node of the dense layer is a parameter of the function.\n",
    "\n",
    "The function must return the entire autoencoder model as well as the encoder and the decoder.\n",
    "You will need the following classes:\n",
    "- [Input](https://keras.io/layers/core/)\n",
    "- [Dense](https://keras.io/layers/core/)\n",
    "- [Model](https://keras.io/models/model/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_simple_autoencoder(encoding_dim=32):\n",
    "    input_img = Input(shape=(784,))\n",
    "    encoded = Dense(encoding_dim, activation='relu')(input_img)\n",
    "    decoded = Dense(784, activation='sigmoid')(encoded)\n",
    "    autoencoder = Model(input_img, decoded)\n",
    "    encoder = Model(input_img, encoded)\n",
    "    encoded_input = Input(shape=(encoding_dim,))\n",
    "    decoder_layer = autoencoder.layers[-1]\n",
    "    decoder = Model(encoded_input, decoder_layer(encoded_input))\n",
    "    autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')   \n",
    "    return autoencoder,encoder,decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Build the autoencoder with a embedding size of 32 and print the number of parameters of the model. What do they relate to ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "autoencoder,encoder,decoder=build_simple_autoencoder(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 784)               25872     \n",
      "=================================================================\n",
      "Total params: 50,992\n",
      "Trainable params: 50,992\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50992"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2*784*32+784+32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Fit the autoencoder using 32 epochs with a batch size of 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "60000/60000 [==============================] - 3s 51us/step - loss: 0.3664 - val_loss: 0.2713\n",
      "Epoch 2/50\n",
      "60000/60000 [==============================] - 3s 46us/step - loss: 0.2640 - val_loss: 0.2530\n",
      "Epoch 3/50\n",
      "60000/60000 [==============================] - 4s 60us/step - loss: 0.2426 - val_loss: 0.2301\n",
      "Epoch 4/50\n",
      "60000/60000 [==============================] - 3s 55us/step - loss: 0.2223 - val_loss: 0.2120\n",
      "Epoch 5/50\n",
      "60000/60000 [==============================] - 3s 48us/step - loss: 0.2070 - val_loss: 0.1992\n",
      "Epoch 6/50\n",
      "60000/60000 [==============================] - 3s 57us/step - loss: 0.1957 - val_loss: 0.1893\n",
      "Epoch 7/50\n",
      "60000/60000 [==============================] - 3s 57us/step - loss: 0.1869 - val_loss: 0.1815\n",
      "Epoch 8/50\n",
      "34048/60000 [================>.............] - ETA: 1s - loss: 0.1811"
     ]
    }
   ],
   "source": [
    "autoencoder.fit(x_train, x_train,\n",
    "                epochs=50,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. Using the history module of the autoencoder write a function that plots the learning curves with respect to the epochs on the train and test set. What can you say about these learning curves ? Give also the last loss on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_learning_curves(autoencoder):\n",
    "    history=autoencoder.history\n",
    "    # summarize history for loss\n",
    "    plt.figure(figsize=(7,7))\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_learning_curves(autoencoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5. Write a function that plots a fix number of example of the original images on the test as well as their reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nearest neighbours graphs\n",
    "The goal of this part is to visualize the neighbours graph in the embedding. It corresponds the the graph of the k-nearest neighbours using the euclidean distance of points the element in the embedding\n",
    "\n",
    "The function that computes the neighbors graphs can be found here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import kneighbors_graph\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_nearest_neighbour_graph(encoder,x_test,y_test,ntest=100,p=3): #to explain\n",
    "    X=encoder.predict(x_test[1:ntest])\n",
    "    y=y_test[1:ntest]\n",
    "    A = kneighbors_graph(X, p, mode='connectivity', include_self=True)\n",
    "    G=nx.from_numpy_array(A.toarray())\n",
    "    nx.set_node_attributes(G,dict(zip(range(ntest),y)),'attr')\n",
    "    fig, ax = plt.subplots(figsize=(10,10))\n",
    "    pos=nx.layout.kamada_kawai_layout(G)\n",
    "    nx.draw(G,pos=pos\n",
    "            ,with_labels=True\n",
    "            ,labels=nx.get_node_attributes(G,'attr')\n",
    "            ,node_color=graph_colors(G))\n",
    "    plt.tight_layout()\n",
    "    plt.title('Nearest Neighbours Graph',fontsize=15)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce the dimension of the embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6. Rerun the previous example using an embedding dimension of 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding sparsity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7.  In this part we will add sparisity over the weights on the embedding layer. Write a function that build such a autoencoder (using a l1 regularization with a configurable regularization parameter and using the same autoencoder architecture that before)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutionnal autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application to denoising"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
