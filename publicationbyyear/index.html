<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.8.1 by Michael Rose
  Copyright 2017 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE.txt
-->
<html lang="fr" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin SEO -->









<title>Publication - Titouan Vayer</title>




<meta name="description" content="Personal website and blog.">




<meta name="author" content="Titouan Vayer">

<meta property="og:locale" content="fr">
<meta property="og:site_name" content="Titouan Vayer">
<meta property="og:title" content="Publication">


  <link rel="canonical" href="http://localhost:4000/publicationbyyear/">
  <meta property="og:url" content="http://localhost:4000/publicationbyyear/">

















  <meta name="twitter:site" content="@tvayer">
  <meta name="twitter:title" content="Publication">
  <meta name="twitter:description" content="Personal website and blog.">
  <meta name="twitter:url" content="http://localhost:4000/publicationbyyear/">

  
    <meta name="twitter:card" content="summary">
    
  

  
    <meta name="twitter:creator" content="@Titouan Vayer">
  



  

  












  <script type="application/ld+json">
    {
      "@context" : "http://schema.org",
      "@type" : "Person",
      "name" : "Titouan Vayer",
      "url" : "http://localhost:4000",
      "sameAs" : null
    }
  </script>







<!-- end SEO -->


<link href="http://localhost:4000/feed.xml" type="application/atom+xml" rel="alternate" title="Titouan Vayer Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- Load jQuery -->
<script src="//code.jquery.com/jquery-1.11.1.min.js"></script>
<!-- Load KaTeX -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0-beta/katex.min.css" integrity="sha384-L/SNYu0HM7XECWBeshTGLluQO9uVI1tvkCtunuoUbCHHoTH76cDyXty69Bb9I0qZ" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0-beta/katex.min.js" integrity="sha384-ad+n9lzhJjYgO67lARKETJH6WuQVDDlRfj81AJJSswMyMkXTD49wBj5EP004WOY6" crossorigin="anonymous"></script>

<!-- For all browsers -->
<link rel="stylesheet" href="http://localhost:4000/assets/css/main.css">

<!--[if lte IE 9]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->


    <!-- start custom head snippets -->

<!-- insert favicons. use http://realfavicongenerator.net/ -->

<!-- end custom head snippets -->
  </head>

  <body class="layout--single">

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    <div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        <a class="site-title" href="http://localhost:4000/">Titouan Vayer</a>
        <ul class="visible-links">
          
            
            <li class="masthead__menu-item">
              <a href="http://localhost:4000/bio" >Bio</a>
            </li>
          
            
            <li class="masthead__menu-item">
              <a href="http://localhost:4000/publicationbyyear" >Publication</a>
            </li>
          
            
            <li class="masthead__menu-item">
              <a href="http://localhost:4000/research" >Research</a>
            </li>
          
            
            <li class="masthead__menu-item">
              <a href="http://localhost:4000/teaching" >Teaching</a>
            </li>
          
            
            <li class="masthead__menu-item">
              <a href="http://localhost:4000/blog" >Blog</a>
            </li>
          
        </ul>
        
        <button class="search__toggle" type="button">
          <svg class="icon" width="16" height="16" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 15.99 16">
            <path d="M15.5,13.12L13.19,10.8a1.69,1.69,0,0,0-1.28-.55l-0.06-.06A6.5,6.5,0,0,0,5.77,0,6.5,6.5,0,0,0,2.46,11.59a6.47,6.47,0,0,0,7.74.26l0.05,0.05a1.65,1.65,0,0,0,.5,1.24l2.38,2.38A1.68,1.68,0,0,0,15.5,13.12ZM6.4,2A4.41,4.41,0,1,1,2,6.4,4.43,4.43,0,0,1,6.4,2Z" transform="translate(-.01)"></path>
          </svg>
        </button>
        
        <button class="greedy-nav__toggle" type="button">
          <span class="visually-hidden">Toggle Menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>

    <div class="initial-content">
      



<div id="main" role="main">
  
  <div class="sidebar sticky">
  

<div itemscope itemtype="http://schema.org/Person">

  
    <div class="author__avatar">
      

      
        <img src="http://localhost:4000/assets/images/me3.png" alt="Titouan Vayer" itemprop="image">
      
    </div>
  

  <div class="author__content">
    
      <h3 class="author__name" itemprop="name">Titouan Vayer</h3>
    
    
      <p class="author__bio" itemprop="description">
        Postdoctoral researcher at ENS Lyon
      </p>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Contact</button>
    <ul class="author__urls social-icons">
      

      

      
        <li>
          <a href="mailto:titouan.vayer@ens-lyon.fr">
            <meta itemprop="email" content="titouan.vayer@ens-lyon.fr" />
            <i class="fa fa-fw fa-envelope-square" aria-hidden="true"></i> Email
          </a>
        </li>
      

      

      
        <li>
          <a href="https://twitter.com/t_vayer" itemprop="sameAs">
            <i class="fa fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter
          </a>
        </li>
      

      

      

      

      

      

      

      

      
        <li>
          <a href="https://github.com/tvayer" itemprop="sameAs">
            <i class="fa fa-fw fa-github" aria-hidden="true"></i> GitHub
          </a>
        </li>
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs">
      <i class="fa fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
  </div>


  <article class="page" itemscope itemtype="http://schema.org/CreativeWork">
    <meta itemprop="headline" content="Publication">
    
    
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 class="page__title" itemprop="headline">Publication
</h1>
          
        </header>
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fa fa-file-text"></i> Sur cette page</h4></header>
              <ul class="toc__menu">
  <li><a href="#">Preprints</a></li>
  <li><a href="#2022">2022</a></li>
  <li><a href="#2021">2021</a></li>
  <li><a href="#2020">2020</a></li>
  <li><a href="#2019">2019</a></li>
</ul>
            </nav>
          </aside>
        
        <p> 
You can also find my articles on my 
<a href= "https://scholar.google.fr/citations?user=PJEv3JgAAAAJ&hl=fr"> Google Scholar profile</a>.  
</p> 
<p>
* Indicates equal contribution.
</p> 
<!-- <a href="/publicationbytopic/"><button type="button" class="btn" style="outline:none">By Topic </button></a> 
 --><a href="/publicationbyyear/"><button type="button" class="btn" style="outline:none">By Year  </button></a> 

<h3  class="pubyear">Preprints</h3>
<ol class="bibliography"><li><span id="Vaysketch"><b>Vayer, T</b>., &amp; Gribonval, R. (2021). <i>Controlling Wasserstein distances by Kernel norms with application to Compressive Statistical Learning</i>.</span>


    
    <button class="btn btnId btnPub--abstract" id="b_Vaysketch-abstract" style="outline:none;">Abstract</button>
    

	<button class="btn btnId btnPub--BibTex" id="b_Vaysketch-bibtex" style="outline:none;">BibTeX</button>
    
    
	
	
	
	
	<a href="https://hal.archives-ouvertes.fr/hal-03461492"><button class="btn btnId btnPub--supplement" style="outline:none; position:relative;white-space: normal;">URL</button></a>
    

	<div class="dropDownBibtex" id="Vaysketch-bibtex">
    <pre>@article{Vaysketch,
  title = {{Controlling Wasserstein distances by Kernel norms with application to Compressive Statistical Learning}},
  author = {Vayer, Titouan and Gribonval, Rémi},
  year = {2021},
  note = {https://hal.archives-ouvertes.fr/hal-03461492},
  archiveprefix = {arXiv},
  presort = {13}
}
</pre>
    </div>
	
	
	<div class="dropDownAbstract" id="Vaysketch-abstract">
    Comparing probability distributions is at the crux of many machine learning algorithms. Maximum Mean Discrepancies (MMD) and Optimal Transport distances (OT) are two classes of distances between probability measures that have attracted abundant attention in past years. This paper establishes some conditions under which the Wasserstein distance can be controlled by MMD norms. Our work is motivated by the compressive statistical learning (CSL) theory, a general framework for resource-efficient large scale learning in which the training data is summarized in a single vector (called sketch) that captures the information relevant to the considered learning task. Inspired by existing results in CSL, we introduce the Hölder Lower Restricted Isometric Property (Hölder LRIP) and show that this property comes with interesting guarantees for compressive statistical learning. Based on the relations between the MMD and the Wasserstein distance, we provide guarantees for compressive statistical learning by introducing and studying the concept of Wasserstein learnability of the learning task, that is when some task-specific metric between probability distributions can be bounded by a Wasserstein distance.
    </div>
	
</li>
<li><span id="Vaytimeseries"><b>Vayer, T</b>., Chapel, L., Courty, N., Flamary, R., Soullard, Y., &amp; Tavenard, R. (2020). <i>Time Series Alignment with Global Invariances</i>.</span>


    
    <button class="btn btnId btnPub--abstract" id="b_Vaytimeseries-abstract" style="outline:none;">Abstract</button>
    

	<button class="btn btnId btnPub--BibTex" id="b_Vaytimeseries-bibtex" style="outline:none;">BibTeX</button>
    
    
	
	
	
	
	<a href="https://hal.archives-ouvertes.fr/hal-02473959"><button class="btn btnId btnPub--supplement" style="outline:none; position:relative;white-space: normal;">URL</button></a>
    

	<div class="dropDownBibtex" id="Vaytimeseries-bibtex">
    <pre>@article{Vaytimeseries,
  title = {Time Series Alignment with Global Invariances},
  author = {Vayer, Titouan and Chapel, Laetitia and Courty, Nicolas and Flamary, Rémi and Soullard, Yann and Tavenard, Romain},
  year = {2020},
  note = {https://hal.archives-ouvertes.fr/hal-02473959}
}
</pre>
    </div>
	
	
	<div class="dropDownAbstract" id="Vaytimeseries-abstract">
    In this work we address the problem of comparing time series while taking into account both feature space transformation and temporal variability. The proposed framework combines a latent global transformation of the feature space with the widely used Dynamic Time Warping (DTW). The latent global transformation captures the feature invariance while the DTW (or its smooth counterpart soft-DTW) deals with the temporal shifts. We cast the problem as a joint optimization over the global transformation and the temporal alignments. The versatility of our framework allows for several variants depending on the invariance class at stake. Among our contributions we define a differentiable loss for time series and present two algorithms for the computation of time series barycenters under our new geometry. We illustrate the interest of our approach on both simulated and real world data.
    </div>
	
</li></ol>


  <h3  id="2022" class="pubyear">2022</h3>
  <ol class="bibliography"><li><span id="Vaysemirelaxed">Vincent-Cuaz, C., Flamary, R., Corneli, M., <b>Vayer, T</b>., &amp; Courty, N. (2022). Semi-relaxed Gromov-Wasserstein divergence and applications on graphs. <i>International Conference on Learning Representations (ICLR)</i>.</span>


    
    <button class="btn btnId btnPub--abstract" id="b_Vaysemirelaxed-abstract" style="outline:none;">Abstract</button>
    

	<button class="btn btnId btnPub--BibTex" id="b_Vaysemirelaxed-bibtex" style="outline:none;">BibTeX</button>
    
    
	
	
	
	
	<a href="https://openreview.net/forum?id=RShaMexjc-x"><button class="btn btnId btnPub--supplement" style="outline:none; position:relative;white-space: normal;">URL</button></a>
    

	<div class="dropDownBibtex" id="Vaysemirelaxed-bibtex">
    <pre>@inproceedings{Vaysemirelaxed,
  title = {{Semi-relaxed Gromov-Wasserstein divergence and applications on graphs}},
  author = {Vincent-Cuaz, C{\'e}dric and Flamary, R{\'e}mi and Corneli, Marco and Vayer, Titouan and Courty, Nicolas},
  booktitle = {International Conference on Learning Representations (ICLR)},
  year = {2022},
  note = {https://openreview.net/forum?id=RShaMexjc-x},
  address = {Online}
}
</pre>
    </div>
	
	
	<div class="dropDownAbstract" id="Vaysemirelaxed-abstract">
    Comparing structured objects such as graphs is a fundamental operation
involved in many learning tasks. To this end, the Gromov-Wasserstein (GW)
distance, based on Optimal Transport (OT), has proven to be successful in
handling the specific nature of the associated objects. More specifically,
through the nodes connectivity relations, GW operates on graphs, seen as
probability measures over specific spaces. At the core of OT is the idea of
conservation of mass, which imposes a coupling between all the nodes from
the two considered graphs. We argue in this paper that this property can be
detrimental for tasks such as graph dictionary or partition learning, and we
relax it by proposing a new semi-relaxed Gromov-Wasserstein divergence.
Aside from immediate computational benefits, we discuss its properties, and
show that it can lead to an efficient graph dictionary learning algorithm.
We empirically demonstrate its relevance for complex tasks on graphs such as
partitioning, clustering and completion.
    </div>
	
</li>
<li><span id="Vaymultiscale">Marcotte, S., Barbe, A., Gribonval, R., <b>Vayer, T</b>., Sebban, M., Borgnat, P., &amp; Gonçalves, P. (2022, May). Fast Multiscale Diffusion on Graphs. <i>International Conference on Acoustics, Speech and Signal Processing (ICASSP)</i>.</span>


    
    <button class="btn btnId btnPub--abstract" id="b_Vaymultiscale-abstract" style="outline:none;">Abstract</button>
    

	<button class="btn btnId btnPub--BibTex" id="b_Vaymultiscale-bibtex" style="outline:none;">BibTeX</button>
    
    
	
	
	
	
	<a href="https://hal.archives-ouvertes.fr/hal-03212764"><button class="btn btnId btnPub--supplement" style="outline:none; position:relative;white-space: normal;">URL</button></a>
    

	<div class="dropDownBibtex" id="Vaymultiscale-bibtex">
    <pre>@inproceedings{Vaymultiscale,
  title = {{Fast Multiscale Diffusion on Graphs}},
  author = {Marcotte, Sibylle and Barbe, Amélie and Gribonval, Rémi and Vayer, Titouan and Sebban, Marc and Borgnat, Pierre and Gonçalves, Paulo},
  year = {2022},
  booktitle = {International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  note = {https://hal.archives-ouvertes.fr/hal-03212764},
  address = {Singapore, Singapore},
  month = may
}
</pre>
    </div>
	
	
	<div class="dropDownAbstract" id="Vaymultiscale-abstract">
    Diffusing a graph signal at multiple scales requires computing the action of the exponential of several multiples of the Laplacian matrix. We tighten a bound on the approximation error of truncated Chebyshev polynomial approximations of the exponential, hence significantly improving a priori estimates of the polynomial order for a prescribed error. We further exploit properties of these approximations to factorize the computation of the action of the diffusion operator over multiple scales, thus reducing drastically its computational cost.
    </div>
	
</li></ol>

  <h3  id="2021" class="pubyear">2021</h3>
  <ol class="bibliography"><li><span id="Vaysubspace">Bonet, C., <b>Vayer, T</b>., Courty, N., Septier, F., &amp; Drumetz, L. (2021). Subspace Detours Meet Gromov–Wasserstein. <i>Algorithms</i>, <i>14</i>(12), 366.</span>


    
    <button class="btn btnId btnPub--abstract" id="b_Vaysubspace-abstract" style="outline:none;">Abstract</button>
    

	<button class="btn btnId btnPub--BibTex" id="b_Vaysubspace-bibtex" style="outline:none;">BibTeX</button>
    
    
	
	
	
	
	<a href="http://dx.doi.org/10.3390/a14120366"><button class="btn btnId btnPub--supplement" style="outline:none; position:relative;white-space: normal;">URL</button></a>
    

	<div class="dropDownBibtex" id="Vaysubspace-bibtex">
    <pre>@article{Vaysubspace,
  author = {Bonet, Clément and Vayer, Titouan and Courty, Nicolas and Septier, François and Drumetz, Lucas},
  title = {{Subspace Detours Meet Gromov–Wasserstein}},
  journal = {Algorithms},
  volume = {14},
  year = {2021},
  number = {12},
  issn = {1999-4893},
  note = {http://dx.doi.org/10.3390/a14120366},
  month = dec,
  pages = {366}
}
</pre>
    </div>
	
	
	<div class="dropDownAbstract" id="Vaysubspace-abstract">
    In the context of optimal transport (OT) methods, the subspace detour approach was recently proposed by Muzellec and Cuturi. It consists of first finding an optimal plan between the measures projected on a wisely chosen subspace and then completing it in a nearly optimal transport plan on the whole space. The contribution of this paper is to extend this category of methods to the Gromov–Wasserstein problem, which is a particular type of OT distance involving the specific geometry of each distribution. After deriving the associated formalism and properties, we give an experimental illustration on a shape matching problem. We also discuss a specific cost for which we can show connections with the Knothe–Rosenblatt rearrangement.
    </div>
	
</li>
<li><span id="Vaypot">Flamary*, R., Courty*, N., Gramfort*, A., Alaya, M. Z., Boisbunon, A., Chambon, S., Chapel, L., Corenflos, A., Fatras, K., Fournier, N., Gautheron, L., Gayraud, N. T. H., Janati, H., Rakotomamonjy, A., Redko, I., Rolet, A., Schutz, A., Seguy, V., Sutherland, D. J., … <b>Vayer, T</b>. (2021). POT: Python Optimal Transport. <i>Journal of Machine Learning Research (JMLR)</i>, <i>22</i>(78), 1–8.</span>


    
    <button class="btn btnId btnPub--abstract" id="b_Vaypot-abstract" style="outline:none;">Abstract</button>
    

	<button class="btn btnId btnPub--BibTex" id="b_Vaypot-bibtex" style="outline:none;">BibTeX</button>
    
    
	
	
	
	
	<a href="http://jmlr.org/papers/v22/20-451.html"><button class="btn btnId btnPub--supplement" style="outline:none; position:relative;white-space: normal;">URL</button></a>
    

	<div class="dropDownBibtex" id="Vaypot-bibtex">
    <pre>@article{Vaypot,
  author = {Flamary*, R\'{e}mi and Courty*, Nicolas and Gramfort*, Alexandre and Alaya, Mokhtar Z. and Boisbunon, Aur\'{e}lie and Chambon, Stanislas and Chapel, Laetitia and Corenflos, Adrien and Fatras, Kilian and Fournier, Nemo and Gautheron, L\'{e}o and Gayraud, Nathalie T.H. and Janati, Hicham and Rakotomamonjy, Alain and Redko, Ievgen and Rolet, Antoine and Schutz, Antony and Seguy, Vivien and Sutherland, Danica J. and Tavenard, Romain and Tong, Alexander and Vayer, Titouan},
  title = {{POT: Python Optimal Transport}},
  note = {http://jmlr.org/papers/v22/20-451.html},
  journal = {Journal of Machine Learning Research (JMLR)},
  year = {2021},
  volume = {22},
  number = {78},
  pages = {1-8},
  presort = {02}
}
</pre>
    </div>
	
	
	<div class="dropDownAbstract" id="Vaypot-abstract">
    Optimal transport has recently been reintroduced to the machine learning community thanks in part to novel efficient optimization procedures allowing for medium to large scale applications. We propose a Python toolbox that implements several key optimal transport ideas for the machine learning community. The toolbox contains implementations of a number of founding works of OT for machine learning such as Sinkhorn algorithm and Wasserstein barycenters, but also provides generic solvers that can be used for conducting novel fundamental research. This toolbox, named POT for Python Optimal Transport, is open source with an MIT license.
    </div>
	
</li>
<li><span id="Vaydiff">Barbe, A., Gonçalves, P., Sebban, M., Borgnat, P., Gribonval, R., &amp; <b>Vayer, T</b>. (2021). Optimization of the Diffusion Time in Graph Diffused-Wasserstein Distances: Application to Domain Adaptation. <i>IEEE 33rd International Conference on Tools with Artificial Intelligence (ICTAI)</i>, 786–790.</span>


    
    <button class="btn btnId btnPub--abstract" id="b_Vaydiff-abstract" style="outline:none;">Abstract</button>
    

	<button class="btn btnId btnPub--BibTex" id="b_Vaydiff-bibtex" style="outline:none;">BibTeX</button>
    
    
	
	
	
	

	<div class="dropDownBibtex" id="Vaydiff-bibtex">
    <pre>@inproceedings{Vaydiff,
  title = {{Optimization of the Diffusion Time in Graph Diffused-Wasserstein Distances: Application to Domain Adaptation}},
  author = {Barbe, Am{\'e}lie and Gon{\c c}alves, Paulo and Sebban, Marc and Borgnat, Pierre and Gribonval, R{\'e}mi and Vayer, Titouan},
  booktitle = {{IEEE 33rd International Conference on Tools with Artificial Intelligence (ICTAI)}},
  year = {2021},
  volume = {},
  number = {},
  pages = {786-790},
  address = {Online}
}
</pre>
    </div>
	
	
	<div class="dropDownAbstract" id="Vaydiff-abstract">
    The use of the heat kernel on graphs has recently given rise to a family of so-called Diffusion-Wasserstein distances which resort to Optimal Transport theory for comparing attributed graphs. In this paper, we address the open problem of optimizing the diffusion time used in these distances. Inspired from the notion of triplet-based constraints, we design a loss function that aims at bringing two graphs closer together while keeping an impostor away. After a thorough analysis of the properties of this function, we show on synthetic data that the resulting Diffusion-Wasserstein distances outperforms the Gromov and Fused-Gromov Wasserstein distances on unsupervised graph domain adaptation tasks.
    </div>
	
</li>
<li><span id="Vaygdl">Vincent-Cuaz, C., <b>Vayer, T</b>., Flamary, R., Corneli, M., &amp; Courty, N. (2021). Online Graph Dictionary Learning. <i>International Conference on Machine Learning (ICML)</i>, <i>139</i>, 10564–10574.</span>


    
    <button class="btn btnId btnPub--abstract" id="b_Vaygdl-abstract" style="outline:none;">Abstract</button>
    

	<button class="btn btnId btnPub--BibTex" id="b_Vaygdl-bibtex" style="outline:none;">BibTeX</button>
    
    
	
	
	
	
	<a href="https://proceedings.mlr.press/v139/vincent-cuaz21a.html"><button class="btn btnId btnPub--supplement" style="outline:none; position:relative;white-space: normal;">URL</button></a>
    

	<div class="dropDownBibtex" id="Vaygdl-bibtex">
    <pre>@inproceedings{Vaygdl,
  title = {{Online Graph Dictionary Learning}},
  author = {Vincent-Cuaz, C{\'e}dric and Vayer, Titouan and Flamary, R{\'e}mi and Corneli, Marco and Courty, Nicolas},
  booktitle = {International Conference on Machine Learning (ICML)},
  year = {2021},
  pages = {10564--10574},
  volume = {139},
  address = {Online},
  series = {Proceedings of Machine Learning Research},
  month = {18--24 Jul},
  publisher = {PMLR},
  note = {https://proceedings.mlr.press/v139/vincent-cuaz21a.html}
}
</pre>
    </div>
	
	
	<div class="dropDownAbstract" id="Vaygdl-abstract">
    Dictionary learning is a key tool for representation learning, that explains the data as linear combination of few basic elements. Yet, this analysis is not amenable in the context of graph learning, as graphs usually belong to different metric spaces. We fill this gap by proposing a new online Graph Dictionary Learning approach, which uses the Gromov Wasserstein divergence for the data fitting term. In our work, graphs are encoded through their nodes’ pairwise relations and modeled as convex combination of graph atoms, i.e. dictionary elements, estimated thanks to an online stochastic algorithm, which operates on a dataset of unregistered graphs with potentially different number of nodes. Our approach naturally extends to labeled graphs, and is completed by a novel upper bound that can be used as a fast approximation of Gromov Wasserstein in the embedding space. We provide numerical evidences showing the interest of our approach for unsupervised embedding of graph datasets and for online graph subspace estimation and tracking.

    </div>
	
</li></ol>

  <h3  id="2020" class="pubyear">2020</h3>
  <ol class="bibliography"><li><span id="Vayerfgwalgorithms"><b>Vayer, T</b>., Chapel, L., Flamary, R., Tavenard, R., &amp; Courty, N. (2020). Fused Gromov-Wasserstein distance for structured objects. <i>Algorithms</i>, <i>13</i>(9), 212. https://doi.org/10.3390/a13090212</span>


    

	<button class="btn btnId btnPub--BibTex" id="b_Vayerfgwalgorithms-bibtex" style="outline:none;">BibTeX</button>
    
    
	
	
	
	
	<a href="http://dx.doi.org/10.3390/a13090212"><button class="btn btnId btnPub--supplement" style="outline:none; position:relative;white-space: normal;">URL</button></a>
    

	<div class="dropDownBibtex" id="Vayerfgwalgorithms-bibtex">
    <pre>@article{Vayerfgwalgorithms,
  title = {{Fused Gromov-Wasserstein distance for structured objects}},
  author = {Vayer, Titouan and Chapel, Laetitia and Flamary, R{\'e}mi and Tavenard, Romain and Courty, Nicolas},
  journal = {Algorithms},
  volume = {13},
  number = {9},
  pages = {212},
  year = {2020},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {1999-4893},
  note = {http://dx.doi.org/10.3390/a13090212},
  doi = {10.3390/a13090212},
  month = aug
}
</pre>
    </div>
	
	
	<div class="dropDownAbstract" id="Vayerfgwalgorithms-abstract">
    
    </div>
	
</li>
<li><span id="Vaycoot"><b>Vayer*, T</b>., Redko*, I., Flamary*, R., &amp; Courty*, N. (2020). CO-Optimal Transport. <i> Neural Information Processing Systems (NeurIPS)</i>, <i>33</i>.</span>


    
    <button class="btn btnId btnPub--abstract" id="b_Vaycoot-abstract" style="outline:none;">Abstract</button>
    

	<button class="btn btnId btnPub--BibTex" id="b_Vaycoot-bibtex" style="outline:none;">BibTeX</button>
    
    
	
	
	
	
	<a href="https://papers.nips.cc/paper/2020/hash/cc384c68ad503482fb24e6d1e3b512ae-Abstract.html"><button class="btn btnId btnPub--supplement" style="outline:none; position:relative;white-space: normal;">URL</button></a>
    

	<div class="dropDownBibtex" id="Vaycoot-bibtex">
    <pre>@inproceedings{Vaycoot,
  author = {Vayer*, Titouan and Redko*, Ievgen and Flamary*, R\'{e}mi and Courty*, Nicolas},
  booktitle = { Neural Information Processing Systems (NeurIPS)},
  title = {{CO-Optimal Transport}},
  year = {2020},
  address = {Online, Canada},
  month = dec,
  volume = {33},
  note = {https://papers.nips.cc/paper/2020/hash/cc384c68ad503482fb24e6d1e3b512ae-Abstract.html}
}
</pre>
    </div>
	
	
	<div class="dropDownAbstract" id="Vaycoot-abstract">
    Optimal transport (OT) is a powerful geometric and probabilistic tool for finding correspondences and measuring similarity between two distributions. Yet, its original formulation relies on the existence of a cost function between the samples of the two distributions, which makes it impractical when they are supported on different spaces. To circumvent this limitation, we propose a novel OT problem, named COOT for CO-Optimal Transport, that simultaneously optimizes two transport maps between both samples and features, contrary to other approaches that either discard the individual features by focusing on pairwise distances between samples or need to model explicitly the relations between them. We provide a thorough theoretical analysis of our problem, establish its rich connections with other OT-based distances and demonstrate its versatility with two machine learning applications in heterogeneous domain adaptation and co-clustering/data summarization, where COOT leads to performance improvements over the state-of-the-art methods.

    </div>
	
</li></ol>

  <h3  id="2019" class="pubyear">2019</h3>
  <ol class="bibliography"><li><span id="Vaysgw"><b>Vayer, T</b>., Flamary, R., Courty, N., Tavenard, R., &amp; Chapel, L. (2019). Sliced Gromov-Wasserstein. <i>Neural Information Processing Systems (NeurIPS)</i>, <i>32</i>.</span>


    
    <button class="btn btnId btnPub--abstract" id="b_Vaysgw-abstract" style="outline:none;">Abstract</button>
    

	<button class="btn btnId btnPub--BibTex" id="b_Vaysgw-bibtex" style="outline:none;">BibTeX</button>
    
    
	
	
	
	
	<a href="https://papers.nips.cc/paper/9615-sliced-gromov-wasserstein"><button class="btn btnId btnPub--supplement" style="outline:none; position:relative;white-space: normal;">URL</button></a>
    

	<div class="dropDownBibtex" id="Vaysgw-bibtex">
    <pre>@inproceedings{Vaysgw,
  author = {Vayer, Titouan and Flamary, R\'{e}mi and Courty, Nicolas and Tavenard, Romain and Chapel, Laetitia},
  booktitle = {Neural Information Processing Systems (NeurIPS)},
  title = {{Sliced Gromov-Wasserstein}},
  year = {2019},
  address = {Vancouver, Canada},
  volume = {32},
  note = {https://papers.nips.cc/paper/9615-sliced-gromov-wasserstein}
}
</pre>
    </div>
	
	
	<div class="dropDownAbstract" id="Vaysgw-abstract">
    Recently used in various machine learning contexts, the Gromov-Wasserstein distance (GW) allows for comparing distributions whose supports do not necessarily lie in the same metric space. However, this Optimal Transport (OT) distance requires solving a complex non convex quadratic program which is most of the time very costly both in time and memory. Contrary to GW, the Wasserstein distance (W) enjoys several properties (\em e.g. duality) that permit large scale optimization. Among those, the solution of W on the real line, that only requires sorting discrete samples in 1D, allows defining the Sliced Wasserstein (SW) distance. This paper proposes a new divergence based on GW akin to SW. We first derive a closed form for GW when dealing with 1D distributions, based on a new result for the related quadratic assignment problem. We then define a novel OT discrepancy that can deal with large scale distributions via a slicing approach and we show how it relates to the GW distance while being O(n\log(n)) to compute. We illustrate the behavior of this so called Sliced Gromov-Wasserstein (SGW) discrepancy in experiments where we demonstrate its ability to tackle similar problems as GW while being several order of magnitudes faster to compute.

    </div>
	
</li>
<li><span id="Vayfgwicml"><b>Vayer, T</b>., Courty, N., Tavenard, R., Chapel, L., &amp; Flamary, R. (2019). Optimal Transport for structured data with application on graphs. <i>International Conference on Machine Learning (ICML)</i>, <i>97</i>, 6275–6284.</span>


    
    <button class="btn btnId btnPub--abstract" id="b_Vayfgwicml-abstract" style="outline:none;">Abstract</button>
    

	<button class="btn btnId btnPub--BibTex" id="b_Vayfgwicml-bibtex" style="outline:none;">BibTeX</button>
    
    
	
	
	
	
	<a href="https://proceedings.mlr.press/v97/titouan19a.html"><button class="btn btnId btnPub--supplement" style="outline:none; position:relative;white-space: normal;">URL</button></a>
    

	<div class="dropDownBibtex" id="Vayfgwicml-bibtex">
    <pre>@inproceedings{Vayfgwicml,
  title = {{Optimal Transport for structured data with application on graphs}},
  author = {Vayer, Titouan and Courty, Nicolas and Tavenard, Romain and Chapel, Laetitia and Flamary, R{\'e}mi},
  booktitle = {International Conference on Machine Learning (ICML)},
  year = {2019},
  pages = {6275--6284},
  volume = {97},
  series = {Proceedings of Machine Learning Research},
  month = {09--15 Jun},
  address = {Long Beach, USA},
  note = {https://proceedings.mlr.press/v97/titouan19a.html}
}
</pre>
    </div>
	
	
	<div class="dropDownAbstract" id="Vayfgwicml-abstract">
    This work considers the problem of computing distances between structured objects such as undirected graphs, seen as probability distributions in a specific metric space. We consider a new transportation distance ( i.e. that minimizes a total cost of transporting probability masses) that unveils the geometric nature of the structured objects space. Unlike Wasserstein or Gromov-Wasserstein metrics that focus solely and respectively on features (by considering a metric in the feature space) or structure (by seeing structure as a metric space), our new distance exploits jointly both information, and is consequently called Fused Gromov-Wasserstein (FGW). After discussing its properties and computational aspects, we show results on a graph classification task, where our method outperforms both graph kernels and deep graph convolutional networks. Exploiting further on the metric properties of FGW, interesting geometric objects such as Fréchet means or barycenters of graphs are illustrated and discussed in a clustering context.
    </div>
	
</li></ol>




        
      </section>

      <footer class="page__meta">
        
        


        
      </footer>

      

      
    </div>

    
  </article>

  
  
</div>

    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap">
  <input type="text" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  <div id="results" class="results"></div>
</div>
      </div>
    

    <div class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Contact</strong></li>
    
    
      <li><a href="https://twitter.com/tvayer"><i class="fa fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
    
    
    
      <li><a href="https://github.com/tvayer"><i class="fa fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
    
    
    
    <li><a href="http://localhost:4000/feed.xml"><i class="fa fa-fw fa-rss-square" aria-hidden="true"></i> Flux</a></li>
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2022 Titouan Vayer. Propulsé par <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

<!-- AJax for DropDown Effect of Abstract and BibTex -->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>
<script>
$(document).ready(function(){
	var str =$(this).attr('id');
	
    $(".btnId").click(function(){
        var str = $(this).attr('id');
		var ret = str.split("_");
		var id = ret[1];
		$('#' + id).toggle();
    });
});
</script>

<!-- New to convert MathJax to KaTex 
<script type="text/javascript">
$("script[type='math/tex']").replaceWith(
  function(){
    var tex = $(this).text();
    return "<span class=\"inline-equation\">" + 
           katex.renderToString(tex) +
           "</span>";
});

$("script[type='math/tex; mode=display']").replaceWith(
  function(){
    var tex = $(this).text();
    return "<div class=\"equation\">" + 
           katex.renderToString("\\displaystyle "+tex) +
           "</div>";
});
</script>
-->

<!-- Previous for changing MathJax to KaTex -->
<script>
  $("script[type='math/tex']").replaceWith(function() {
      var tex = $(this).text();
      return katex.renderToString(tex.replace(/%.*/g, ''), {displayMode: false});
  });

  $("script[type='math/tex; mode=display']").replaceWith(function() {
      var tex = $(this).html();
      return katex.renderToString(tex.replace(/%.*/g, ''), {displayMode: true});
  });
</script>

      </footer>
    </div>

    
  <script src="http://localhost:4000/assets/js/main.min.js"></script>



  <script src="http://localhost:4000/assets/js/lunr.min.js"></script>
  <script src="http://localhost:4000/assets/js/lunr-en.js"></script>






  </body>
</html>