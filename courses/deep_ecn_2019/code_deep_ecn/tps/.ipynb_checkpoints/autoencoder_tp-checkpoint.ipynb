{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "//anaconda/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcol\n",
    "from matplotlib import cm\n",
    "def graph_colors(nx_graph):\n",
    "    #cm1 = mcol.LinearSegmentedColormap.from_list(\"MyCmapName\",[\"blue\",\"red\"])\n",
    "    #cm1 = mcol.Colormap('viridis')\n",
    "\n",
    "    cnorm = mcol.Normalize(vmin=0,vmax=9)\n",
    "    cpick = cm.ScalarMappable(norm=cnorm,cmap='Set1')\n",
    "    cpick.set_array([])\n",
    "    val_map = {}\n",
    "    for k,v in nx.get_node_attributes(nx_graph,'attr').items():\n",
    "        #print(v)\n",
    "        val_map[k]=cpick.to_rgba(v)\n",
    "    #print(val_map)\n",
    "    colors=[]\n",
    "    for node in nx_graph.nodes():\n",
    "        #print(node,val_map.get(str(node), 'black'))\n",
    "        colors.append(val_map[node])\n",
    "    return colors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1 Write a function that builds a simple autoencoder \n",
    "\n",
    "The autoencoder must have a simple Dense layer with relu activation. The number of node of the dense layer is a parameter of the function\n",
    "\n",
    "The function must return the entire autoencoder model as well as the encoder and the decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load the mnist dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Buil the autoencoder with a embedding size of 32 and print the number of parameters of the model. What do they relate to ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Fit the autoencoder using 32 epochs with a batch size of 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. Using the history module of the autoencoder write a function that plots the learning curves with respect to the epochs on the train and test set. What can you say about these learning curves ? Give also the last loss on the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5. Write a function that plots a fix number of example of the original images on the test as weel as their reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nearest neighbours graphs\n",
    "The goal of this part is to visualize the neighbours graph in the embedding. It corresponds the the graph of the k-nearest neighbours using the euclidean distance of points the element in the embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import kneighbors_graph\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_nearest_neighbour_graph(encoder,x_test,y_test,ntest=100,p=3): #to explain\n",
    "    X=encoder.predict(x_test[1:ntest])\n",
    "    y=y_test[1:ntest]\n",
    "    A = kneighbors_graph(X, p, mode='connectivity', include_self=True)\n",
    "    G=nx.from_numpy_array(A.toarray())\n",
    "    nx.set_node_attributes(G,dict(zip(range(ntest),y)),'attr')\n",
    "    fig, ax = plt.subplots(figsize=(10,10))\n",
    "    pos=nx.layout.kamada_kawai_layout(G)\n",
    "    nx.draw(G,pos=pos\n",
    "            ,with_labels=True\n",
    "            ,labels=nx.get_node_attributes(G,'attr')\n",
    "            ,node_color=graph_colors(G))\n",
    "    plt.tight_layout()\n",
    "    plt.title('Nearest Neighbours Graph',fontsize=15)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce the dimension of the embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6. Rerun the previous example using an embedding dimension of 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding sparsity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7.  In this part we will add sparisity over the weights on the embedding layer. Write a function that build such a autoencoder (using a l1 regularization with a configurable regularization parameter and using the same autoencoder architecture that before)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutionnal autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application to denoising"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
